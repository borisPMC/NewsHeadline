{"cells":[{"cell_type":"markdown","metadata":{},"source":["> This notebook is trying to use [A Million News Headlines](https://www.kaggle.com/therohk/million-headlines) dataset to implement a fake news headlines detection model using machine learning approach. Where the A Million News Headlines dataset will be labeled as real news headlines. And this notebook will also use two fake news headline datasets on Kaggle from [Fake and real news](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset) and [Getting Real about Fake News](https://www.kaggle.com/mrisdal/fake-news) . \n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Load Datasets**"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["#Read Dataset\n","Headlines = pd.read_csv('dataset/abcnews-date-text.csv', usecols =[\"headline_text\"]).dropna()\n","Headlines1 = pd.read_csv('dataset/fake.csv', usecols =[\"title\"]).dropna()\n","# Headlines2 = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv', usecols =[\"title\"]).dropna()"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["#Removing duplicated headlines\n","Headlines = Headlines.drop_duplicates('headline_text')\n","Headlines1 = Headlines1.drop_duplicates('title')\n","# Headlines2 = Headlines2.drop_duplicates('title')"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["#Rename dataframe columns and Combine all datasets\n","Headlines1 = Headlines1.rename(columns={'title': 'headline_text'})\n","# Headlines2 = Headlines2.rename(columns={'title': 'headline_text'})"]},{"cell_type":"markdown","metadata":{},"source":["# **Labelling**"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["#Creating lable for datasets\n","#million-headlines dataset will be used as real headlines\n","#fake-and-real-news-dataset & fake-news dataset will be used as fake headlines\n","Headlines['fake'] = 0\n","Headlines1['fake'] = 1\n","# Headlines2['fake'] = 1"]},{"cell_type":"markdown","metadata":{},"source":["# **Combine Datasets**"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset contains: 50000 Real headlines and 11698 Fake headlines.\n"]}],"source":["#Downsize million-headlines dataset to first 50K rows\n","data = pd.concat([Headlines[:50000],Headlines1])\n","print('Training dataset contains: {} Real headlines and {} Fake headlines.'.format(50000,len(Headlines1)))\n","data.to_csv('Combined_headlines.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Preprocessing**"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to ./nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to ./nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import gensim\n","import nltk as nl\n","nl.download(\"stopwords\", download_dir='./nltk_data')\n","nl.download(\"punkt\", download_dir='./nltk_data')\n","from sklearn.feature_extraction import text\n","\n","\n","nltk_stopwords = nl.corpus.stopwords.words('english')\n","gensim_stopwords = gensim.parsing.preprocessing.STOPWORDS\n","sklearn_stopwords = text.ENGLISH_STOP_WORDS\n","combined_stopwords = sklearn_stopwords.union(nltk_stopwords,gensim_stopwords)"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK has 179 stop words\n","Gensim has 337 stop words\n","Sklearn has 318 stop words\n","Combined stopwords list has 390 stop words\n"]}],"source":["print('NLTK has {} stop words'.format(len(nltk_stopwords)))\n","print('Gensim has {} stop words'.format(len(gensim_stopwords)))\n","print('Sklearn has {} stop words'.format(len(sklearn_stopwords)))\n","print('Combined stopwords list has {} stop words'.format(len(combined_stopwords)))"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["from nltk.stem import PorterStemmer \n","porter_stemmer = PorterStemmer() "]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["data['headline_text'] = data['headline_text'].apply(lambda x: x.lower())\n","data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n","data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([porter_stemmer.stem(word) for word in x.split()]))\n","data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (combined_stopwords)]))"]},{"cell_type":"markdown","metadata":{},"source":["# **Splitting Dataset**"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","x_train,x_test,y_train,y_test=train_test_split(data['headline_text'], data['fake'], test_size=0.2, random_state=7)"]},{"cell_type":"markdown","metadata":{},"source":["# **Construct models with TF-IDF**"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Conv1D, MaxPooling1D, Flatten, Embedding, GlobalMaxPooling1D\n","# from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}],"source":["tfidf_vectorizer = TfidfVectorizer(tokenizer = word_tokenize, max_features = 300)\n","tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n","tfidf_test = tfidf_vectorizer.transform(x_test)\n","tfidf_features = tfidf_vectorizer.get_feature_names_out()"]},{"cell_type":"markdown","metadata":{},"source":["** RandomForest Classifier- Randomized Search**"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n","90 fits failed out of a total of 250.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","17 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n","    estimator._validate_params()\n","  File \"c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n","    validate_parameter_constraints(\n","  File \"c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n","\n","--------------------------------------------------------------------------------\n","73 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n","    estimator._validate_params()\n","  File \"c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n","    validate_parameter_constraints(\n","  File \"c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","c:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.81143887        nan 0.81376874 0.83139532        nan\n"," 0.81046639        nan        nan 0.825621   0.81794234 0.82404051\n"," 0.82953123 0.81052717 0.81952258        nan 0.82341261        nan\n"," 0.81046639 0.81046639        nan 0.81046639 0.84142387        nan\n"," 0.82231872 0.81046639 0.82266289        nan        nan 0.81709154\n","        nan 0.83688576        nan 0.81810435 0.81052717        nan\n"," 0.81046639 0.81780037 0.81243159 0.81393084 0.81046639 0.81267472\n","        nan 0.81050691        nan        nan 0.81046639 0.81048665\n","        nan 0.81279631]\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 56}\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import randint\n","\n","# Define the hyperparameter distributions\n","param_dist = {\n","    'n_estimators': randint(10, 100),\n","    'max_depth': randint(1, 10),\n","    'min_samples_split': randint(2, 20),\n","    'min_samples_leaf': randint(1, 20),\n","    'max_features': ['auto', 'sqrt', 'log2']\n","}\n","\n","# Initialize the Random Forest model\n","rf = RandomForestClassifier()\n","\n","# Set up RandomizedSearchCV\n","rand_search = RandomizedSearchCV(\n","    estimator=rf,\n","    param_distributions=param_dist,\n","    n_iter=50, # Number of parameter settings that are sampled\n","    cv=5, # Number of cross-validation folds\n","    verbose=2, # Controls the verbosity: the higher, the more messages\n","    random_state=42, # For reproducibility\n","    n_jobs=-1 # Use all processors\n",")\n","\n","# Fit the model to the data\n","rand_search.fit(tfidf_train, y_train)\n","\n","# Access the best model and its hyperparameters\n","best_rf = rand_search.best_estimator_\n","print('Best hyperparameters:', rand_search.best_params_)\n"]},{"cell_type":"markdown","metadata":{},"source":["** Logistic Regression using Random Sampling-Balance Training Set**"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"zero-dimensional arrays cannot be concatenated","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m class_0_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m additional_class_0_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(class_0_indices, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m balanced_train_x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43madditional_class_0_samples\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m balanced_train_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((y_train, y_train[additional_class_0_samples]))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create a logistic regression model with balanced class weights\u001b[39;00m\n","\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","\n","class_0_indices = np.where(y_train == 0)[0]\n","additional_class_0_samples = np.random.choice(class_0_indices, size=1000, replace=False)\n","balanced_train_x = np.concatenate((tfidf_train, tfidf_train[additional_class_0_samples]))\n","balanced_train_y = np.concatenate((y_train, y_train[additional_class_0_samples]))\n","\n","# Create a logistic regression model with balanced class weights\n","log_clf_balanced = LogisticRegression(penalty='none', class_weight='balanced', solver='saga', random_state=42)\n","log_clf_balanced.fit(balanced_train_x, balanced_train_y)\n","\n","# Perform cross-validation\n","scores = cross_val_score(log_clf_balanced, balanced_train_x, balanced_train_y, cv=5)\n","\n","print(\"Cross-validated scores:\", scores)\n","print(\"Mean cross-validated score:\", scores.mean())\n"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"],"text/plain":["MultinomialNB()"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["dt = DecisionTreeClassifier()\n","rf = RandomForestClassifier()\n","svc = SVC(kernel='linear')\n","knn = KNeighborsClassifier()\n","nb = MultinomialNB()\n","\n","dt.fit(tfidf_train, y_train)\n","rf.fit(tfidf_train, y_train)\n","svc.fit(tfidf_train, y_train)\n","knn.fit(tfidf_train, y_train)\n","nb.fit(tfidf_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","log_clf_lasso = LogisticRegression(C = 0.1, class_weight= 'balanced', penalty= 'l1', solver= 'liblinear',random_state=42)\n","log_clf_lasso.fit(tfidf_train,y_train)\n","crossvalscore(log_clf_lasso)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing Acc. of Decision Tree: 87.32 %\n","Testing Acc. of Random Forest: 88.13 %\n","Testing Acc. of SVC: 87.63 %\n","Testing Acc. of K-NN: 80.23 %\n","Testing Acc. of Naive Bayesian: 88.03 %\n"]}],"source":["print (\"Testing Acc. of Decision Tree: {} %\".format(round(dt.score(tfidf_test, y_test) * 100, 2)))\n","print (\"Testing Acc. of Random Forest: {} %\".format(round(rf.score(tfidf_test, y_test) * 100, 2)))\n","print (\"Testing Acc. of SVC: {} %\".format(round(svc.score(tfidf_test, y_test) * 100, 2)))\n","print (\"Testing Acc. of K-NN: {} %\".format(round(knn.score(tfidf_test, y_test) * 100, 2)))\n","print (\"Testing Acc. of Naive Bayesian: {} %\".format(round(nb.score(tfidf_test, y_test) * 100, 2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\borisPMC\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Testing Acc. of DNN: 88.2 %\n"]}],"source":["tfidf_train = tfidf_train.todense()\n","tfidf_test = tfidf_test.todense()\n","\n","neural_network = Sequential()\n","neural_network.add(Dense(64, input_dim=len(tfidf_features), activation='relu'))\n","neural_network.add(Dropout(0.1))\n","neural_network.add(Dense(64, activation='relu'))\n","neural_network.add(Dropout(0.1))\n","neural_network.add(Dense(1, activation='sigmoid'))\n","neural_network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history = neural_network.fit(tfidf_train, y_train, epochs=50, batch_size=512, verbose=0)\n","_,test_acc = neural_network.evaluate(tfidf_test,y_test,verbose=0)\n","print (\"Testing Acc. of DNN: {} %\".format(round(test_acc * 100, 2)))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":444,"sourceId":911,"sourceType":"datasetVersion"},{"datasetId":572515,"sourceId":1037534,"sourceType":"datasetVersion"},{"datasetId":1692,"sourceId":3782728,"sourceType":"datasetVersion"}],"dockerImageVersionId":30042,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
