{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8b9d6583-6ae1-4bb6-970c-c80d42dfebdb","_uuid":"d56b7961b91d7f67554ec48bd1d2485fb7419120"},"source":["# About the dataset\n","This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.)\n","Site: http://www.abc.net.au/\n","Prepared by Rohit Kulkarni\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.feature_extraction import text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem.snowball import SnowballStemmer\n"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"jupyter":{"outputs_hidden":true},"scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>publish_date</th>\n","      <th>headline_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>20030219</td>\n","      <td>aba decides against community broadcasting lic...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20030219</td>\n","      <td>act fire witnesses must be aware of defamation</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>20030219</td>\n","      <td>a g calls for infrastructure protection summit</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>20030219</td>\n","      <td>air nz staff in aust strike for pay rise</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20030219</td>\n","      <td>air nz strike to affect australian travellers</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   publish_date                                      headline_text\n","0      20030219  aba decides against community broadcasting lic...\n","1      20030219     act fire witnesses must be aware of defamation\n","2      20030219     a g calls for infrastructure protection summit\n","3      20030219           air nz staff in aust strike for pay rise\n","4      20030219      air nz strike to affect australian travellers"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#data = pd.read_csv(\"abcnews-date-text.csv\", error_bad_lines=False,usecols =[\"headline_text\"])\n","data = pd.read_csv(\"abcnews-date-text.csv\")\n","data.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"3e44d856-a323-45ac-b7cc-80d77385060f","_uuid":"a498ee778ab763e0801b8f9cf14e1d4d01f38846","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1244184 entries, 0 to 1244183\n","Data columns (total 2 columns):\n"," #   Column         Non-Null Count    Dtype \n","---  ------         --------------    ----- \n"," 0   publish_date   1244184 non-null  int64 \n"," 1   headline_text  1244184 non-null  object\n","dtypes: int64(1), object(1)\n","memory usage: 19.0+ MB\n"]}],"source":["data.info()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ff887c6d-0470-4f62-860b-9457b223bb8c","_uuid":"eb590852f097f66ea53be9a970789430fc3f6a63"},"source":["# Deleting dupliate headlines(if any)"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"42392880-315c-41bf-98e8-a1cbfab72f6e","_uuid":"1e0143660cbb59acf14ed07c847fd9bc3ca85045","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>publish_date</th>\n","      <th>headline_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>116298</th>\n","      <td>20040920</td>\n","      <td>10 killed in pakistan bus crash</td>\n","    </tr>\n","    <tr>\n","      <th>57967</th>\n","      <td>20031129</td>\n","      <td>10 killed in pakistan bus crash</td>\n","    </tr>\n","    <tr>\n","      <th>911080</th>\n","      <td>20141023</td>\n","      <td>110 with barry nicholls</td>\n","    </tr>\n","    <tr>\n","      <th>672958</th>\n","      <td>20120217</td>\n","      <td>110 with barry nicholls</td>\n","    </tr>\n","    <tr>\n","      <th>748629</th>\n","      <td>20121214</td>\n","      <td>110 with barry nicholls</td>\n","    </tr>\n","    <tr>\n","      <th>676423</th>\n","      <td>20120302</td>\n","      <td>110 with barry nicholls</td>\n","    </tr>\n","    <tr>\n","      <th>897042</th>\n","      <td>20140820</td>\n","      <td>110 with barry nicholls episode 15</td>\n","    </tr>\n","    <tr>\n","      <th>826828</th>\n","      <td>20131017</td>\n","      <td>110 with barry nicholls episode 15</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        publish_date                       headline_text\n","116298      20040920     10 killed in pakistan bus crash\n","57967       20031129     10 killed in pakistan bus crash\n","911080      20141023             110 with barry nicholls\n","672958      20120217             110 with barry nicholls\n","748629      20121214             110 with barry nicholls\n","676423      20120302             110 with barry nicholls\n","897042      20140820  110 with barry nicholls episode 15\n","826828      20131017  110 with barry nicholls episode 15"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"9f5ff611-397e-45b7-9616-7bc33f6e81bb","_uuid":"4e5e82d7c6fb0e7b14f8b5772bea14e448f88fcc","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["data = data.drop_duplicates('headline_text')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"f1bb8d35-27aa-4ff4-9a39-4329517aa6a4","_uuid":"f022fdf6441499ed52b34c063240f4f28b2ff3a5"},"source":["# NLP "]},{"cell_type":"markdown","metadata":{"_cell_guid":"7ecba7c7-84d0-426b-aa9c-9ca05d45da75","_uuid":"d2f1e1b88fb7b29fd47c249be3af044ef1e2a246"},"source":["# Preparing data for vectorizaion\n","However, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. This process is sometimes referred to as “embedding” or “vectorization”.\n","\n","In terms of vectorization, it is important to remember that it isn’t merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Not only can a vector have more than one dimension, but with text data vectors are usually high-dimensional. This is because each dimension of your feature data will correspond to a word, and the language in the documents you are examining will have thousands of words.\n","\n","# TF-IDF\n","In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.\n","\n","Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n","\n","One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"c7e595ab-440c-4ad7-98e4-4358cc724d8c","_uuid":"9c1c23ecabae8217a9aa8f90371f2a30053cc6f1","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"ename":"InvalidParameterError","evalue":"The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'have', 'seemed', 'thru', 'next', 'yourselves', 'by', 'whereas', 'whom', 'their', 'latterly', 'become', 'side', 'move', 'un', 'at', 'can', 'everything', 'back', 'anywhere', 'yet', 'beyond', 'sixty', 'a', 'during', 'hereby', 'beside', 'part', 'anyhow', 'third', ':', 'what', 'least', 'through', ']', '%', 'call', 'twelve', 'up', 'formerly', 'such', 'below', 'besides', 'some', 'down', 'yourself', 'meanwhile', 'con', 'nor', 'every', 'of', 'with', 'would', 'cannot', 'neither', 'your', 'here', 'another', 'beforehand', 'together', 'enough', 'might', 'one', 'fifty', 'wherein', 'alone', 'keep', 'whereupon', 'etc', 'co', 'else', 'whereby', 'therefore', 'whether', 'upon', 'everyone', 'among', 'into', 'other', 'thin', 'mine', 'front', 'towards', 'then', 'any', '{', 'whoever', 'former', 'myself', 'last', 'becoming', 'off', 'system', 'while', 'get', 'ever', 'mill', 'as', 'either', 'for', 'where', 'thick', 'us', 'rather', 'not', 'out', 'seeming', 'across', 'few', 'seems', 'ie', 'why', 'itself', 'fifteen', 'is', 'five', 'sometime', 'our', 'she', 'an', 'somehow', 'that', 'two', 'toward', 'also', 'made', 'see', 'thereafter', 'whence', 'more', 'thereby', 'to', 'after', 'most', 'amongst', 'whereafter', 'do', 'these', 'anyway', ';', 'them', 'he', 'you', 'forty', 'if', 'done', 'hundred', 'i', 'hasnt', 'cry', 'interest', 'was', 'since', 'thence', 'something', 'detail', 'twenty', 'against', 'they', 'put', 'this', 'nothing', 'will', 'him', 'we', 'hereupon', 'hence', 'ltd', 'but', 'namely', 'afterwards', 'above', 'may', 'empty', 'around', 'anything', 'behind', 'whole', 'anyone', 'its', 'per', 'under', 'both', 'fire', 'someone', 'who', 'my', 'are', '}', 'there', 'be', 'others', 'along', 'already', 'herein', 'again', 'too', 'bottom', 'nowhere', 'always', 'ourselves', 'has', 'latter', 'am', 'whenever', 'de', 'many', 'otherwise', 'me', 'whatever', 'give', 'therein', 'show', 'wherever', 'no', 'must', 'only', 'themselves', 'hereafter', 'now', 'until', 'how', 'less', 'been', 'find', 'over', 'than', 'often', 'however', 'noone', 'himself', 'and', 'those', 'being', 'never', 'serious', 'amount', 'becomes', 'her', 'still', 'yours', 'thereupon', 'could', 're', '\"', '!', 'herself', 'his', 'became', 'much', 'within', 'sincere', 'three', 'well', 'own', 'even', 'eight', 'eg', 'each', ')', 'hers', 'please', 'so', 'without', 'about', 'six', 'cant', 'describe', 'from', 'further', 'nobody', 'everywhere', \"'\", 'throughout', 'somewhere', 'because', 'via', 'except', 'couldnt', 'none', 'indeed', 'the', 'all', 'sometimes', 'nine', 'first', 'on', 'very', 'ten', 'mostly', 'four', 'eleven', 'inc', 'or', '.', 'ours', 'amoungst', 'bill', 'several', 'name', 'seem', 'moreover', 'take', 'nevertheless', 'whither', 'top', 'found', 'full', 'same', 'between', 'before', 'once', 'although', 'almost', 'it', 'when', ',', 'go', '[', 'were', 'had', 'though', 'whose', 'should', '?', 'due', 'perhaps', 'which', 'thus', 'fill', '(', 'onto', 'elsewhere', 'in'}) instead.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m desc \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words \u001b[38;5;241m=\u001b[39m stop_words)\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1141\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1142\u001b[0m )\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1145\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 638\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m )\n","\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'have', 'seemed', 'thru', 'next', 'yourselves', 'by', 'whereas', 'whom', 'their', 'latterly', 'become', 'side', 'move', 'un', 'at', 'can', 'everything', 'back', 'anywhere', 'yet', 'beyond', 'sixty', 'a', 'during', 'hereby', 'beside', 'part', 'anyhow', 'third', ':', 'what', 'least', 'through', ']', '%', 'call', 'twelve', 'up', 'formerly', 'such', 'below', 'besides', 'some', 'down', 'yourself', 'meanwhile', 'con', 'nor', 'every', 'of', 'with', 'would', 'cannot', 'neither', 'your', 'here', 'another', 'beforehand', 'together', 'enough', 'might', 'one', 'fifty', 'wherein', 'alone', 'keep', 'whereupon', 'etc', 'co', 'else', 'whereby', 'therefore', 'whether', 'upon', 'everyone', 'among', 'into', 'other', 'thin', 'mine', 'front', 'towards', 'then', 'any', '{', 'whoever', 'former', 'myself', 'last', 'becoming', 'off', 'system', 'while', 'get', 'ever', 'mill', 'as', 'either', 'for', 'where', 'thick', 'us', 'rather', 'not', 'out', 'seeming', 'across', 'few', 'seems', 'ie', 'why', 'itself', 'fifteen', 'is', 'five', 'sometime', 'our', 'she', 'an', 'somehow', 'that', 'two', 'toward', 'also', 'made', 'see', 'thereafter', 'whence', 'more', 'thereby', 'to', 'after', 'most', 'amongst', 'whereafter', 'do', 'these', 'anyway', ';', 'them', 'he', 'you', 'forty', 'if', 'done', 'hundred', 'i', 'hasnt', 'cry', 'interest', 'was', 'since', 'thence', 'something', 'detail', 'twenty', 'against', 'they', 'put', 'this', 'nothing', 'will', 'him', 'we', 'hereupon', 'hence', 'ltd', 'but', 'namely', 'afterwards', 'above', 'may', 'empty', 'around', 'anything', 'behind', 'whole', 'anyone', 'its', 'per', 'under', 'both', 'fire', 'someone', 'who', 'my', 'are', '}', 'there', 'be', 'others', 'along', 'already', 'herein', 'again', 'too', 'bottom', 'nowhere', 'always', 'ourselves', 'has', 'latter', 'am', 'whenever', 'de', 'many', 'otherwise', 'me', 'whatever', 'give', 'therein', 'show', 'wherever', 'no', 'must', 'only', 'themselves', 'hereafter', 'now', 'until', 'how', 'less', 'been', 'find', 'over', 'than', 'often', 'however', 'noone', 'himself', 'and', 'those', 'being', 'never', 'serious', 'amount', 'becomes', 'her', 'still', 'yours', 'thereupon', 'could', 're', '\"', '!', 'herself', 'his', 'became', 'much', 'within', 'sincere', 'three', 'well', 'own', 'even', 'eight', 'eg', 'each', ')', 'hers', 'please', 'so', 'without', 'about', 'six', 'cant', 'describe', 'from', 'further', 'nobody', 'everywhere', \"'\", 'throughout', 'somewhere', 'because', 'via', 'except', 'couldnt', 'none', 'indeed', 'the', 'all', 'sometimes', 'nine', 'first', 'on', 'very', 'ten', 'mostly', 'four', 'eleven', 'inc', 'or', '.', 'ours', 'amoungst', 'bill', 'several', 'name', 'seem', 'moreover', 'take', 'nevertheless', 'whither', 'top', 'found', 'full', 'same', 'between', 'before', 'once', 'although', 'almost', 'it', 'when', ',', 'go', '[', 'were', 'had', 'though', 'whose', 'should', '?', 'due', 'perhaps', 'which', 'thus', 'fill', '(', 'onto', 'elsewhere', 'in'}) instead."]}],"source":["punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n","stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n","desc = data['headline_text'].values\n","vectorizer = TfidfVectorizer(stop_words = stop_words)\n","X = vectorizer.fit_transform(desc)"]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"eb56971e-5412-4138-a4cd-0e14844796be","_uuid":"10af64e15e2f08c30da71b847432eedc2aece199","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"ename":"AttributeError","evalue":"'TfidfVectorizer' object has no attribute 'get_feature_names'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_features \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_features))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_features[\u001b[38;5;241m5000\u001b[39m:\u001b[38;5;241m5100\u001b[39m])\n","\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"]}],"source":["word_features = vectorizer.get_feature_names()\n","print(len(word_features))\n","print(word_features[5000:5100])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"871b1bd6-c411-4ff6-a784-9b376a0db4e6","_uuid":"e480f5b88938660f05c09f75af5f9f58d7110096"},"source":["# Stemming\n","Stemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word.\n","\n","# Tokenizing\n","Tokenization is breaking the sentence into words and punctuation,"]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"536a1a88-48a3-43d0-b368-ccf31947e5b1","_uuid":"5d25104db183624b990a1d64e10cc618fd8ee715","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["stemmer = SnowballStemmer('english')\n","tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n","\n","def tokenize(text):\n","    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9877cf33-ebe8-46e4-b26e-c6f673617517","_uuid":"d190c4d6b9cb52ad70a021e3475704538b47f85f"},"source":["# Vectorization with stop words(words irrelevant to the model), stemming and tokenizing"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"18e1d30c-5515-4c0d-89e4-6af99658bee3","_uuid":"5fcc93fa3093f30d181ffa38b4ca46b133316952","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"ename":"InvalidParameterError","evalue":"The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'have', 'seemed', 'thru', 'next', 'yourselves', 'by', 'whereas', 'whom', 'their', 'latterly', 'become', 'side', 'move', 'un', 'at', 'can', 'everything', 'back', 'anywhere', 'yet', 'beyond', 'sixty', 'a', 'during', 'hereby', 'beside', 'part', 'anyhow', 'third', ':', 'what', 'least', 'through', ']', '%', 'call', 'twelve', 'up', 'formerly', 'such', 'below', 'besides', 'some', 'down', 'yourself', 'meanwhile', 'con', 'nor', 'every', 'of', 'with', 'would', 'cannot', 'neither', 'your', 'here', 'another', 'beforehand', 'together', 'enough', 'might', 'one', 'fifty', 'wherein', 'alone', 'keep', 'whereupon', 'etc', 'co', 'else', 'whereby', 'therefore', 'whether', 'upon', 'everyone', 'among', 'into', 'other', 'thin', 'mine', 'front', 'towards', 'then', 'any', '{', 'whoever', 'former', 'myself', 'last', 'becoming', 'off', 'system', 'while', 'get', 'ever', 'mill', 'as', 'either', 'for', 'where', 'thick', 'us', 'rather', 'not', 'out', 'seeming', 'across', 'few', 'seems', 'ie', 'why', 'itself', 'fifteen', 'is', 'five', 'sometime', 'our', 'she', 'an', 'somehow', 'that', 'two', 'toward', 'also', 'made', 'see', 'thereafter', 'whence', 'more', 'thereby', 'to', 'after', 'most', 'amongst', 'whereafter', 'do', 'these', 'anyway', ';', 'them', 'he', 'you', 'forty', 'if', 'done', 'hundred', 'i', 'hasnt', 'cry', 'interest', 'was', 'since', 'thence', 'something', 'detail', 'twenty', 'against', 'they', 'put', 'this', 'nothing', 'will', 'him', 'we', 'hereupon', 'hence', 'ltd', 'but', 'namely', 'afterwards', 'above', 'may', 'empty', 'around', 'anything', 'behind', 'whole', 'anyone', 'its', 'per', 'under', 'both', 'fire', 'someone', 'who', 'my', 'are', '}', 'there', 'be', 'others', 'along', 'already', 'herein', 'again', 'too', 'bottom', 'nowhere', 'always', 'ourselves', 'has', 'latter', 'am', 'whenever', 'de', 'many', 'otherwise', 'me', 'whatever', 'give', 'therein', 'show', 'wherever', 'no', 'must', 'only', 'themselves', 'hereafter', 'now', 'until', 'how', 'less', 'been', 'find', 'over', 'than', 'often', 'however', 'noone', 'himself', 'and', 'those', 'being', 'never', 'serious', 'amount', 'becomes', 'her', 'still', 'yours', 'thereupon', 'could', 're', '\"', '!', 'herself', 'his', 'became', 'much', 'within', 'sincere', 'three', 'well', 'own', 'even', 'eight', 'eg', 'each', ')', 'hers', 'please', 'so', 'without', 'about', 'six', 'cant', 'describe', 'from', 'further', 'nobody', 'everywhere', \"'\", 'throughout', 'somewhere', 'because', 'via', 'except', 'couldnt', 'none', 'indeed', 'the', 'all', 'sometimes', 'nine', 'first', 'on', 'very', 'ten', 'mostly', 'four', 'eleven', 'inc', 'or', '.', 'ours', 'amoungst', 'bill', 'several', 'name', 'seem', 'moreover', 'take', 'nevertheless', 'whither', 'top', 'found', 'full', 'same', 'between', 'before', 'once', 'although', 'almost', 'it', 'when', ',', 'go', '[', 'were', 'had', 'though', 'whose', 'should', '?', 'due', 'perhaps', 'which', 'thus', 'fill', '(', 'onto', 'elsewhere', 'in'}) instead.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer2 \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words \u001b[38;5;241m=\u001b[39m stop_words, tokenizer \u001b[38;5;241m=\u001b[39m tokenize)\n\u001b[1;32m----> 2\u001b[0m X2 \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m word_features2 \u001b[38;5;241m=\u001b[39m vectorizer2\u001b[38;5;241m.\u001b[39mget_feature_names()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_features2))\n","File \u001b[1;32mc:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1141\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1142\u001b[0m )\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1145\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 638\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Oscar Yu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m )\n","\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'have', 'seemed', 'thru', 'next', 'yourselves', 'by', 'whereas', 'whom', 'their', 'latterly', 'become', 'side', 'move', 'un', 'at', 'can', 'everything', 'back', 'anywhere', 'yet', 'beyond', 'sixty', 'a', 'during', 'hereby', 'beside', 'part', 'anyhow', 'third', ':', 'what', 'least', 'through', ']', '%', 'call', 'twelve', 'up', 'formerly', 'such', 'below', 'besides', 'some', 'down', 'yourself', 'meanwhile', 'con', 'nor', 'every', 'of', 'with', 'would', 'cannot', 'neither', 'your', 'here', 'another', 'beforehand', 'together', 'enough', 'might', 'one', 'fifty', 'wherein', 'alone', 'keep', 'whereupon', 'etc', 'co', 'else', 'whereby', 'therefore', 'whether', 'upon', 'everyone', 'among', 'into', 'other', 'thin', 'mine', 'front', 'towards', 'then', 'any', '{', 'whoever', 'former', 'myself', 'last', 'becoming', 'off', 'system', 'while', 'get', 'ever', 'mill', 'as', 'either', 'for', 'where', 'thick', 'us', 'rather', 'not', 'out', 'seeming', 'across', 'few', 'seems', 'ie', 'why', 'itself', 'fifteen', 'is', 'five', 'sometime', 'our', 'she', 'an', 'somehow', 'that', 'two', 'toward', 'also', 'made', 'see', 'thereafter', 'whence', 'more', 'thereby', 'to', 'after', 'most', 'amongst', 'whereafter', 'do', 'these', 'anyway', ';', 'them', 'he', 'you', 'forty', 'if', 'done', 'hundred', 'i', 'hasnt', 'cry', 'interest', 'was', 'since', 'thence', 'something', 'detail', 'twenty', 'against', 'they', 'put', 'this', 'nothing', 'will', 'him', 'we', 'hereupon', 'hence', 'ltd', 'but', 'namely', 'afterwards', 'above', 'may', 'empty', 'around', 'anything', 'behind', 'whole', 'anyone', 'its', 'per', 'under', 'both', 'fire', 'someone', 'who', 'my', 'are', '}', 'there', 'be', 'others', 'along', 'already', 'herein', 'again', 'too', 'bottom', 'nowhere', 'always', 'ourselves', 'has', 'latter', 'am', 'whenever', 'de', 'many', 'otherwise', 'me', 'whatever', 'give', 'therein', 'show', 'wherever', 'no', 'must', 'only', 'themselves', 'hereafter', 'now', 'until', 'how', 'less', 'been', 'find', 'over', 'than', 'often', 'however', 'noone', 'himself', 'and', 'those', 'being', 'never', 'serious', 'amount', 'becomes', 'her', 'still', 'yours', 'thereupon', 'could', 're', '\"', '!', 'herself', 'his', 'became', 'much', 'within', 'sincere', 'three', 'well', 'own', 'even', 'eight', 'eg', 'each', ')', 'hers', 'please', 'so', 'without', 'about', 'six', 'cant', 'describe', 'from', 'further', 'nobody', 'everywhere', \"'\", 'throughout', 'somewhere', 'because', 'via', 'except', 'couldnt', 'none', 'indeed', 'the', 'all', 'sometimes', 'nine', 'first', 'on', 'very', 'ten', 'mostly', 'four', 'eleven', 'inc', 'or', '.', 'ours', 'amoungst', 'bill', 'several', 'name', 'seem', 'moreover', 'take', 'nevertheless', 'whither', 'top', 'found', 'full', 'same', 'between', 'before', 'once', 'although', 'almost', 'it', 'when', ',', 'go', '[', 'were', 'had', 'though', 'whose', 'should', '?', 'due', 'perhaps', 'which', 'thus', 'fill', '(', 'onto', 'elsewhere', 'in'}) instead."]}],"source":["vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n","X2 = vectorizer2.fit_transform(desc)\n","word_features2 = vectorizer2.get_feature_names()\n","print(len(word_features2))\n","print(word_features2[:50]) "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"acdc11f4-7b5c-4aee-8c42-1a752bffbc2e","_uuid":"244a3015e5b4f4f84c174586fa875f5cf49cff1d","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n","X3 = vectorizer3.fit_transform(desc)\n","words = vectorizer3.get_feature_names()"]},{"cell_type":"markdown","metadata":{},"source":["#Split the Data"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'X' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming X is your feature matrix and y is your target vector\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"]}],"source":["from sklearn.model_selection import train_test_split\n","y = data['headline_text'].values\n","# Assuming X is your feature matrix and y is your target vector\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e8a8c1ed-8970-49e0-bae5-0e0d85abea84","_uuid":"b5f2e66c25d17527b78ae1a1fe174f1ac6310286"},"source":["For this, we will use k-means clustering algorithm.\n","# K-means clustering\n","(Source [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm) )\n","![http://gdurl.com/5BbP](http://gdurl.com/5BbP)\""]},{"cell_type":"markdown","metadata":{"_cell_guid":"c9a1312e-45f7-44e4-9407-13012bdf97ce","_uuid":"39ba3fa0f53454111495da2f7e7719572afec93a"},"source":["# Elbow method to select number of clusters\n","This method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.\n","# Basically, number of clusters = the x-axis value of the point that is the corner of the \"elbow\"(the plot looks often looks like an elbow)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c72b26ab-4bef-44e6-b854-5bca3cd1f217","_uuid":"992bea80b2647c4f4e564bb020ce8eab07db6b78","collapsed":true,"jupyter":{"outputs_hidden":true},"scrolled":true},"outputs":[],"source":["from sklearn.cluster import KMeans\n","wcss = []\n","for i in range(1,11):\n","    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n","    kmeans.fit(X3)\n","    wcss.append(kmeans.inertia_)\n","plt.plot(range(1,11),wcss)\n","plt.title('The Elbow Method')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.savefig('elbow.png')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2dd63611-35d6-4c6e-b076-c10e18a3b10c","_uuid":"d5184cf4e47df719970d89c5ea8e15d4b9eaa1e5"},"source":["As more than one elbows have been generated, I will have to select right amount of clusters by trial and error. So, I will showcase the results of different amount of clusters to find out the right amount of clusters."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e096b262-a06c-4f0a-9c50-2ef4bda9b926","_uuid":"0ed982322b3a0fecb997e88ef0fb2f681c5a801c","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["print(words[250:300])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0ca23daa-681a-4cb8-93ae-d2d2ac137604","_uuid":"f463135cb544f6ffa999861c7bf049cbbede8fe6"},"source":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"a3c414e5-70b2-4c63-a46a-f9c82e406e4b","_uuid":"6992f369b10d54adf27ecdcf24c6f57deabf466f"},"source":["# 3 Clusters"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b395dddc-8acb-40a8-825e-a6f3a615809e","_uuid":"e0e187a022ec9032385f715d87cfbb865a11698d","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)\n","kmeans.fit(X3)\n","# We look at 3 the clusters generated by k-means.\n","common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n","for num, centroid in enumerate(common_words):\n","    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"05f80701-83af-49c1-a7b8-df8c20396bb9","_uuid":"4750051c6cbdc6d262f31a94f5c4776eb020644a"},"source":["# 5 Clusters"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"137b7e2c-970f-4f2f-929e-c7b2bd2a8004","_uuid":"2813cee7e61b966b179b806d29b222c55551ee35","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["kmeans = KMeans(n_clusters = 5, n_init = 20, n_jobs = 1)\n","kmeans.fit(X3)\n","# We look at 5 the clusters generated by k-means.\n","common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n","for num, centroid in enumerate(common_words):\n","    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e5ab0cdc-1178-486d-a982-3457aa69d234","_uuid":"4faeeea430f9f679a594b2c7f1ef1c857611ade5"},"source":["# 6 Clusters"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de4d9fbb-2462-4491-899f-b1e28e1a7697","_uuid":"73c1c5bfafe12fd9bd57ab0c3bd07f60630f05a8","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["kmeans = KMeans(n_clusters = 6, n_init = 20, n_jobs = 1)\n","kmeans.fit(X3)\n","# We look at 6 the clusters generated by k-means.\n","common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n","for num, centroid in enumerate(common_words):\n","    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"bff92434-ec9d-4e6b-a4a2-72ec4ea6f3bc","_uuid":"7225da580975f8589615a8fd5232ba3c80780845"},"source":["# 8 Clusters"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4134a6d2-09aa-4cf5-9821-d95d0482c4ac","_uuid":"cf3c5af56aa05f6679effe85774207fe824255a1","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 1)\n","kmeans.fit(X3)\n","# Finally, we look at 8 the clusters generated by k-means.\n","common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n","for num, centroid in enumerate(common_words):\n","    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a168498b-1322-4010-b462-596bc0ba186b","_uuid":"4717568e34623b3b88f033670c20f01f7ddd63a6"},"source":["Because even I didn't know what kind of clusters would be generated, I will describe them in comments."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1692,"sourceId":3782728,"sourceType":"datasetVersion"}],"dockerImageVersionId":283,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
