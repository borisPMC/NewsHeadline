{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":911,"sourceType":"datasetVersion","datasetId":444},{"sourceId":1037534,"sourceType":"datasetVersion","datasetId":572515},{"sourceId":3782728,"sourceType":"datasetVersion","datasetId":1692}],"dockerImageVersionId":30042,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> This notebook is trying to use [A Million News Headlines](https://www.kaggle.com/therohk/million-headlines) dataset to implement a fake news headlines detection model using machine learning approach. Where the A Million News Headlines dataset will be labeled as real news headlines. And this notebook will also use two fake news headline datasets on Kaggle from [Fake and real news](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset) and [Getting Real about Fake News](https://www.kaggle.com/mrisdal/fake-news) . \n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Datasets**","metadata":{}},{"cell_type":"code","source":"#Read Dataset\nHeadlines = pd.read_csv('/kaggle/input/million-headlines/abcnews-date-text.csv', usecols =[\"headline_text\"]).dropna()\nHeadlines1 = pd.read_csv('/kaggle/input/fake-news/fake.csv', usecols =[\"title\"]).dropna()\nHeadlines2 = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv', usecols =[\"title\"]).dropna()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing duplicated headlines\nHeadlines = Headlines.drop_duplicates('headline_text')\nHeadlines1 = Headlines1.drop_duplicates('title')\nHeadlines2 = Headlines2.drop_duplicates('title')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rename dataframe columns and Combine all datasets\nHeadlines1 = Headlines1.rename(columns={'title': 'headline_text'})\nHeadlines2 = Headlines2.rename(columns={'title': 'headline_text'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Labelling**","metadata":{}},{"cell_type":"code","source":"#Creating lable for datasets\n#million-headlines dataset will be used as real headlines\n#fake-and-real-news-dataset & fake-news dataset will be used as fake headlines\nHeadlines['fake'] = 0\nHeadlines1['fake'] = 1\nHeadlines2['fake'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Combine Datasets**","metadata":{}},{"cell_type":"code","source":"#Downsize million-headlines dataset to first 50K rows\ndata = pd.concat([Headlines[:50000],Headlines1,Headlines2])\nprint('Training dataset contains: {} Real headlines and {} Fake headlines.'.format(50000,len(Headlines1)+len(Headlines2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"import gensim\nimport nltk as nl\nfrom sklearn.feature_extraction import stop_words\n\n\nnltk_stopwords = nl.corpus.stopwords.words('english')\ngensim_stopwords = gensim.parsing.preprocessing.STOPWORDS\nsklearn_stopwords = stop_words.ENGLISH_STOP_WORDS\ncombined_stopwords = sklearn_stopwords.union(nltk_stopwords,gensim_stopwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('NLTK has {} stop words'.format(len(nltk_stopwords)))\nprint('Gensim has {} stop words'.format(len(gensim_stopwords)))\nprint('Sklearn has {} stop words'.format(len(sklearn_stopwords)))\nprint('Combined stopwords list has {} stop words'.format(len(combined_stopwords)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import PorterStemmer \nporter_stemmer = PorterStemmer() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['headline_text'] = data['headline_text'].apply(lambda x: x.lower())\ndata['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\ndata['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([porter_stemmer.stem(word) for word in x.split()]))\ndata['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (combined_stopwords)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Splitting Dataset**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(data['headline_text'], data['fake'], test_size=0.2, random_state=7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Construct models with TF-IDF**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Conv1D, MaxPooling1D, Flatten, Embedding, GlobalMaxPooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(tokenizer = word_tokenize, max_features = 300)\ntfidf_train = tfidf_vectorizer.fit_transform(x_train)\ntfidf_test = tfidf_vectorizer.transform(x_test)\ntfidf_features = tfidf_vectorizer.get_feature_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC(kernel='linear')\nknn = KNeighborsClassifier()\nnb = MultinomialNB()\n\ndt.fit(tfidf_train, y_train)\nrf.fit(tfidf_train, y_train)\nsvc.fit(tfidf_train, y_train)\nknn.fit(tfidf_train, y_train)\nnb.fit(tfidf_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Testing Acc. of Decision Tree: {} %\".format(round(dt.score(tfidf_test, y_test) * 100, 2)))\nprint (\"Testing Acc. of Random Forest: {} %\".format(round(rf.score(tfidf_test, y_test) * 100, 2)))\nprint (\"Testing Acc. of SVC: {} %\".format(round(svc.score(tfidf_test, y_test) * 100, 2)))\nprint (\"Testing Acc. of K-NN: {} %\".format(round(knn.score(tfidf_test, y_test) * 100, 2)))\nprint (\"Testing Acc. of Naive Bayesian: {} %\".format(round(nb.score(tfidf_test, y_test) * 100, 2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_train = tfidf_train.todense()\ntfidf_test = tfidf_test.todense()\n\nneural_network = Sequential()\nneural_network.add(Dense(64, input_dim=len(tfidf_features), activation='relu'))\nneural_network.add(Dropout(0.1))\nneural_network.add(Dense(64, activation='relu'))\nneural_network.add(Dropout(0.1))\nneural_network.add(Dense(1, activation='sigmoid'))\nneural_network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = neural_network.fit(tfidf_train, y_train, epochs=50, batch_size=512, verbose=0)\n_,test_acc = neural_network.evaluate(tfidf_test,y_test,verbose=0)\nprint (\"Testing Acc. of DNN: {} %\".format(round(test_acc * 100, 2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}