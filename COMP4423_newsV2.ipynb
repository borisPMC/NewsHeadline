{"cells":[{"cell_type":"markdown","metadata":{},"source":["> This notebook is trying to use [A Million News Headlines](https://www.kaggle.com/therohk/million-headlines) dataset to implement a fake news headlines detection model using machine learning approach. Where the A Million News Headlines dataset will be labeled as real news headlines. And this notebook will also use two fake news headline datasets on Kaggle from [Fake and real news](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset) and [Getting Real about Fake News](https://www.kaggle.com/mrisdal/fake-news) . \n","\n"]},{"cell_type":"code","execution_count":57,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Load Datasets**"]},{"cell_type":"code","execution_count":58,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["#Read Dataset\n","Headlines = pd.read_csv('dataset/abcnews-date-text.csv', usecols =[\"headline_text\"]).dropna()\n","Headlines1 = pd.read_csv('dataset/fake.csv', usecols =[\"title\"]).dropna()\n","# Headlines2 = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv', usecols =[\"title\"]).dropna()"]},{"cell_type":"code","execution_count":59,"metadata":{"trusted":true},"outputs":[],"source":["#Removing duplicated headlines\n","Headlines = Headlines.drop_duplicates('headline_text')\n","Headlines1 = Headlines1.drop_duplicates('title')\n","# Headlines2 = Headlines2.drop_duplicates('title')"]},{"cell_type":"code","execution_count":60,"metadata":{"trusted":true},"outputs":[],"source":["#Rename dataframe columns and Combine all datasets\n","Headlines1 = Headlines1.rename(columns={'title': 'headline_text'})\n","# Headlines2 = Headlines2.rename(columns={'title': 'headline_text'})"]},{"cell_type":"markdown","metadata":{},"source":["# **Labelling**"]},{"cell_type":"code","execution_count":61,"metadata":{"trusted":true},"outputs":[],"source":["#Creating lable for datasets\n","#million-headlines dataset will be used as real headlines\n","#fake-and-real-news-dataset & fake-news dataset will be used as fake headlines\n","Headlines['fake'] = 0\n","Headlines1['fake'] = 1\n","# Headlines2['fake'] = 1"]},{"cell_type":"markdown","metadata":{},"source":["# **Combine Datasets**"]},{"cell_type":"code","execution_count":62,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset contains: 50000 Real headlines and 11698 Fake headlines.\n"]}],"source":["#Downsize million-headlines dataset to first 50K rows\n","data = pd.concat([Headlines[:50000],Headlines1])\n","print('Training dataset contains: {} Real headlines and {} Fake headlines.'.format(50000,len(Headlines1)))"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Preprocessing**"]},{"cell_type":"code","execution_count":63,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to ./nltk_data...\n","[nltk_data]   Unzipping corpora\\stopwords.zip.\n","[nltk_data] Downloading package punkt to ./nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"]}],"source":["import gensim\n","import nltk as nl\n","nl.download(\"stopwords\", download_dir='./nltk_data')\n","nl.download(\"punkt\", download_dir='./nltk_data')\n","from sklearn.feature_extraction import text\n","\n","\n","nltk_stopwords = nl.corpus.stopwords.words('english')\n","gensim_stopwords = gensim.parsing.preprocessing.STOPWORDS\n","sklearn_stopwords = text.ENGLISH_STOP_WORDS\n","combined_stopwords = sklearn_stopwords.union(nltk_stopwords,gensim_stopwords)"]},{"cell_type":"code","execution_count":64,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK has 179 stop words\n","Gensim has 337 stop words\n","Sklearn has 318 stop words\n","Combined stopwords list has 390 stop words\n"]}],"source":["print('NLTK has {} stop words'.format(len(nltk_stopwords)))\n","print('Gensim has {} stop words'.format(len(gensim_stopwords)))\n","print('Sklearn has {} stop words'.format(len(sklearn_stopwords)))\n","print('Combined stopwords list has {} stop words'.format(len(combined_stopwords)))"]},{"cell_type":"code","execution_count":65,"metadata":{"trusted":true},"outputs":[],"source":["from nltk.stem import PorterStemmer \n","porter_stemmer = PorterStemmer() "]},{"cell_type":"code","execution_count":66,"metadata":{"trusted":true},"outputs":[],"source":["data['headline_text'] = data['headline_text'].apply(lambda x: x.lower())\n","data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n","data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([porter_stemmer.stem(word) for word in x.split()]))\n","data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (combined_stopwords)]))"]},{"cell_type":"markdown","metadata":{},"source":["# **Splitting Dataset**"]},{"cell_type":"code","execution_count":67,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","x_train,x_test,y_train,y_test=train_test_split(data['headline_text'], data['fake'], test_size=0.2, random_state=7)"]},{"cell_type":"markdown","metadata":{},"source":["# **Construct models with TF-IDF**"]},{"cell_type":"code","execution_count":68,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Conv1D, MaxPooling1D, Flatten, Embedding, GlobalMaxPooling1D\n","# from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":69,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\borisPMC\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}],"source":["tfidf_vectorizer = TfidfVectorizer(tokenizer = word_tokenize, max_features = 300)\n","tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n","tfidf_test = tfidf_vectorizer.transform(x_test)\n","tfidf_features = tfidf_vectorizer.get_feature_names_out()"]},{"cell_type":"code","execution_count":70,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"],"text/plain":["MultinomialNB()"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["dt = DecisionTreeClassifier()\n","rf = RandomForestClassifier()\n","svc = SVC(kernel='linear')\n","knn = KNeighborsClassifier()\n","nb = MultinomialNB()\n","\n","dt.fit(tfidf_train, y_train)\n","rf.fit(tfidf_train, y_train)\n","svc.fit(tfidf_train, y_train)\n","knn.fit(tfidf_train, y_train)\n","nb.fit(tfidf_train, y_train)"]},{"cell_type":"code","execution_count":71,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing Acc. of Decision Tree: 87.32 %\n","Testing Acc. of Random Forest: 88.13 %\n","Testing Acc. of SVC: 87.63 %\n","Testing Acc. of K-NN: 80.23 %\n","Testing Acc. of Naive Bayesian: 88.03 %\n"]}],"source":["print (\"Testing Acc. of Decision Tree: {} %\".format(round(dt.score(tfidf_test, y_test) * 100, 2)))\n","print (\"Testing Acc. of Random Forest: {} %\".format(round(rf.score(tfidf_test, y_test) * 100, 2)))\n","print (\"Testing Acc. of SVC: {} %\".format(round(svc.score(tfidf_test, y_test) * 100, 2)))\n","print (\"Testing Acc. of K-NN: {} %\".format(round(knn.score(tfidf_test, y_test) * 100, 2)))\n","print (\"Testing Acc. of Naive Bayesian: {} %\".format(round(nb.score(tfidf_test, y_test) * 100, 2)))"]},{"cell_type":"code","execution_count":72,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\borisPMC\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Testing Acc. of DNN: 88.2 %\n"]}],"source":["tfidf_train = tfidf_train.todense()\n","tfidf_test = tfidf_test.todense()\n","\n","neural_network = Sequential()\n","neural_network.add(Dense(64, input_dim=len(tfidf_features), activation='relu'))\n","neural_network.add(Dropout(0.1))\n","neural_network.add(Dense(64, activation='relu'))\n","neural_network.add(Dropout(0.1))\n","neural_network.add(Dense(1, activation='sigmoid'))\n","neural_network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history = neural_network.fit(tfidf_train, y_train, epochs=50, batch_size=512, verbose=0)\n","_,test_acc = neural_network.evaluate(tfidf_test,y_test,verbose=0)\n","print (\"Testing Acc. of DNN: {} %\".format(round(test_acc * 100, 2)))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":444,"sourceId":911,"sourceType":"datasetVersion"},{"datasetId":572515,"sourceId":1037534,"sourceType":"datasetVersion"},{"datasetId":1692,"sourceId":3782728,"sourceType":"datasetVersion"}],"dockerImageVersionId":30042,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
